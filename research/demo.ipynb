{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def detect_and_match_features(image_paths):\n",
    "    # Initialize OpenCV feature detector (SIFT, ORB, etc.) and matcher (Brute Force, FLANN, etc.)\n",
    "    detector = cv2.SIFT_create()\n",
    "    matcher = cv2.BFMatcher()\n",
    "\n",
    "    # Function to detect keypoints and compute descriptors for a single image\n",
    "    def detect_and_compute(image_path):\n",
    "        try:\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if image is None:\n",
    "                print(f\"Error reading image {image_path}. Skipping.\")\n",
    "                return None, None\n",
    "            keypoints, descriptors = detector.detectAndCompute(image, None)\n",
    "            if keypoints is None or descriptors is None:\n",
    "                print(f\"Error detecting features in image {image_path}. Skipping.\")\n",
    "                return None, None\n",
    "            return keypoints, descriptors\n",
    "        except Exception as e:\n",
    "            print(f\"Exception during feature detection: {e}. Skipping {image_path}.\")\n",
    "            return None, None\n",
    "\n",
    "    # Detect keypoints and compute descriptors for all images in parallel\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(detect_and_compute, image_paths))\n",
    "\n",
    "    # Separate keypoints and descriptors from results\n",
    "    keypoints_list, descriptors_list = zip(*results)\n",
    "    keypoints_list = [keypoints for keypoints in keypoints_list if keypoints is not None]\n",
    "    descriptors_list = [descriptors for descriptors in descriptors_list if descriptors is not None]\n",
    "\n",
    "    # Match features between images\n",
    "    matches = [matcher.match(descriptors_list[i], descriptors_list[i + 1]) for i in range(len(descriptors_list) - 1)]\n",
    "\n",
    "    return keypoints_list, matches\n",
    "\n",
    "def find_image_paths(root_dir, scene_category):\n",
    "    image_paths = []\n",
    "    scene_dir = os.path.join(root_dir, scene_category.replace(\" \", \"_\"))\n",
    "    if os.path.isdir(scene_dir):\n",
    "        for subdir, _, files in os.walk(scene_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    image_paths.append(os.path.join(subdir, file))\n",
    "    return image_paths\n",
    "\n",
    "def visualize_matches(image_paths, keypoints_list, matches):\n",
    "    for i, match in enumerate(matches):\n",
    "        try:\n",
    "            # Read images for the current pair of matches\n",
    "            image1 = cv2.imread(image_paths[i], cv2.IMREAD_GRAYSCALE)\n",
    "            image2 = cv2.imread(image_paths[i + 1], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "            # Get keypoints for the current pair of images\n",
    "            keypoints1 = keypoints_list[i]\n",
    "            keypoints2 = keypoints_list[i + 1]\n",
    "\n",
    "            # Draw matches between keypoints on the images\n",
    "            matched_image = cv2.drawMatches(image1, keypoints1, image2, keypoints2, match, None)\n",
    "            \n",
    "            # Display the matched image\n",
    "            plt.figure(figsize=(15, 7))\n",
    "            plt.title(f\"Matched Images {i + 1} to {i + 2}\")\n",
    "            plt.imshow(matched_image)\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Exception during match visualization: {e}. Skipping visualization for image pair {i}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the root directory containing images\n",
    "    root_dir = \"C:/Vishal/Kaggele-Image-Matching-Challenge-2024/data/train\"\n",
    "\n",
    "    # Define the scene categories\n",
    "    scene_categories = [\n",
    "        \"church\",\n",
    "        \"dioscuri\",\n",
    "        \"lizard\",\n",
    "        \"multi-temporal-temple-baalshamin\",\n",
    "        \"pond\",\n",
    "        \"transp_obj_glass_cup\",\n",
    "        \"transp_obj_glass_cylinder\"\n",
    "    ]\n",
    "\n",
    "    # Iterate over each scene category\n",
    "    for scene_category in scene_categories:\n",
    "        print(f\"Processing images for scene category: {scene_category}\")\n",
    "\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Find image paths for the current scene category\n",
    "        image_paths = find_image_paths(root_dir, scene_category)\n",
    "\n",
    "        if not image_paths:\n",
    "            print(\"No images found for this scene category.\")\n",
    "        else:\n",
    "            # Detect and match features using OpenCV\n",
    "            keypoints_list, matches = detect_and_match_features(image_paths)\n",
    "\n",
    "            # Visualize matches\n",
    "            #visualize_matches(image_paths, keypoints_list, matches)\n",
    "\n",
    "        # End timing\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        print(f\"Time taken for scene category '{scene_category}': {elapsed_time:.2f} seconds\")\n",
    "        print(\"------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "def detect_and_match_features(image_paths, scale_factor=0.5, detector_name=\"SIFT\"):\n",
    "    \"\"\"\n",
    "    Detects and matches features for a list of images with optimization options.\n",
    "\n",
    "    Args:\n",
    "        image_paths (list): List of image paths.\n",
    "        scale_factor (float, optional): Factor for image downscaling (default: 0.5).\n",
    "        detector_name (str, optional): Name of the feature detector (default: \"SIFT\").\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing lists of keypoints and matches.\n",
    "    \"\"\"\n",
    "\n",
    "    # Choose detector based on name (can be extended for other detectors)\n",
    "    if detector_name == \"SIFT\":\n",
    "        detector = cv2.SIFT_create()\n",
    "    elif detector_name == \"ORB\":\n",
    "        detector = cv2.ORB_create()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported detector name: {detector_name}\")\n",
    "\n",
    "    matcher = cv2.BFMatcher()\n",
    "\n",
    "    def detect_and_compute(image_path):\n",
    "        try:\n",
    "            # Read and downscale image (if applicable)\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if image is None:\n",
    "                print(f\"Error reading image {image_path}. Skipping.\")\n",
    "                return None, None\n",
    "            image = cv2.resize(image, None, fx=scale_factor, fy=scale_factor, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            # Detect keypoints and compute descriptors\n",
    "            keypoints, descriptors = detector.detectAndCompute(image, None)\n",
    "            if keypoints is None or descriptors is None:\n",
    "                print(f\"Error detecting features in image {image_path}. Skipping.\")\n",
    "                return None, None\n",
    "            return keypoints, descriptors\n",
    "        except Exception as e:\n",
    "            print(f\"Exception during feature detection: {e}. Skipping {image_path}.\")\n",
    "            return None, None\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(detect_and_compute, image_paths))\n",
    "\n",
    "    # Separate keypoints and descriptors from results\n",
    "    # (filter out None values from skipped images)\n",
    "    keypoints_list = [keypoints for keypoints, _ in results if keypoints is not None]\n",
    "    descriptors_list = [descriptors for _, descriptors in results if descriptors is not None]\n",
    "\n",
    "    # Match features between images using brute-force matcher\n",
    "    matches = []\n",
    "    for i in range(len(descriptors_list) - 1):\n",
    "        matches.append(matcher.match(descriptors_list[i], descriptors_list[i + 1]))\n",
    "\n",
    "    return keypoints_list, matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing images for scene category: church\n",
      "Time taken for scene category 'church': 7.91 seconds\n",
      "------------------------------------------------------------------\n",
      "Processing images for scene category: dioscuri\n",
      "Time taken for scene category 'dioscuri': 5.42 seconds\n",
      "------------------------------------------------------------------\n",
      "Processing images for scene category: lizard\n",
      "Time taken for scene category 'lizard': 42.75 seconds\n",
      "------------------------------------------------------------------\n",
      "Processing images for scene category: multi-temporal-temple-baalshamin\n",
      "Time taken for scene category 'multi-temporal-temple-baalshamin': 20.62 seconds\n",
      "------------------------------------------------------------------\n",
      "Processing images for scene category: pond\n",
      "Time taken for scene category 'pond': 117.22 seconds\n",
      "------------------------------------------------------------------\n",
      "Processing images for scene category: transp_obj_glass_cup\n",
      "Time taken for scene category 'transp_obj_glass_cup': 88.67 seconds\n",
      "------------------------------------------------------------------\n",
      "Processing images for scene category: transp_obj_glass_cylinder\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo images found for this scene category.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Detect and match features using OpenCV with FLANN parameter tuning\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m     keypoints_list, matches \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_and_match_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflann_trees\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflann_trees\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflann_checks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflann_checks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# Visualize matches\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# visualize_matches(image_paths, keypoints_list, matches)\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# End timing\u001b[39;00m\n\u001b[0;32m     41\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[1;32mIn[33], line 57\u001b[0m, in \u001b[0;36mdetect_and_match_features\u001b[1;34m(image_paths, scale_factor, detector_name, flann_trees, flann_checks)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m---> 57\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetect_and_compute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_paths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Separate keypoints and descriptors from results\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# (filter out None values from skipped images)\u001b[39;00m\n\u001b[0;32m     61\u001b[0m keypoints_list \u001b[38;5;241m=\u001b[39m [keypoints \u001b[38;5;28;01mfor\u001b[39;00m keypoints, _ \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mif\u001b[39;00m keypoints \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "File \u001b[1;32mc:\\Vishal\\Kaggele-Image-Matching-Challenge-2024\\venv\\lib\\concurrent\\futures\\_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[0;32m    619\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[1;32mc:\\Vishal\\Kaggele-Image-Matching-Challenge-2024\\venv\\lib\\concurrent\\futures\\_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    321\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[1;32mc:\\Vishal\\Kaggele-Image-Matching-Challenge-2024\\venv\\lib\\concurrent\\futures\\_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32mc:\\Vishal\\Kaggele-Image-Matching-Challenge-2024\\venv\\lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main fuction\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the root directory containing images\n",
    "    root_dir = \"C:/Vishal/Kaggele-Image-Matching-Challenge-2024/data/train\"\n",
    "\n",
    "    # Define the scene categories\n",
    "    scene_categories = [\n",
    "        \"church\",\n",
    "        \"dioscuri\",\n",
    "        \"lizard\",\n",
    "        \"multi-temporal-temple-baalshamin\",\n",
    "        \"pond\",\n",
    "        \"transp_obj_glass_cup\",\n",
    "        \"transp_obj_glass_cylinder\"\n",
    "    ]\n",
    "\n",
    "    # Iterate over each scene category\n",
    "    for scene_category in scene_categories:\n",
    "        print(f\"Processing images for scene category: {scene_category}\")\n",
    "\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Find image paths for the current scene category\n",
    "        image_paths = find_image_paths(root_dir, scene_category)\n",
    "\n",
    "        if not image_paths:\n",
    "            print(\"No images found for this scene category.\")\n",
    "        else:\n",
    "            # Detect and match features using OpenCV with brute-force matcher\n",
    "            keypoints_list, matches = detect_and_match_features(image_paths)\n",
    "\n",
    "            # Visualize matches\n",
    "            # visualize_matches(image_paths, keypoints_list, matches)\n",
    "\n",
    "        # End timing\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        print(f\"Time taken for scene category '{scene_category}': {elapsed_time:.2f} seconds\")\n",
    "        print(\"------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_camera_data(camera_file):\n",
    "    \"\"\"Parse camera data from a file.\n",
    "    \n",
    "    Args:\n",
    "        camera_file (str): Path to the camera data file.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing camera data with camera ID as keys.\n",
    "              Each camera entry contains model, width, and height.\n",
    "    \"\"\"\n",
    "    camera_data = {}\n",
    "    try:\n",
    "        with open(camera_file, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines[3:]:  # Skip the first three comment lines\n",
    "                parts = line.strip().split(' ')\n",
    "                camera_id = int(parts[0])\n",
    "                model = parts[1]\n",
    "                width = int(parts[2])\n",
    "                height = int(parts[3])\n",
    "                camera_data[camera_id] = {'model': model, 'width': width, 'height': height}\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{camera_file}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while parsing camera data: {e}\")\n",
    "    return camera_data\n",
    "\n",
    "def parse_image_data(image_file):\n",
    "    \"\"\"Parse image data from a file.\n",
    "    \n",
    "    Args:\n",
    "        image_file (str): Path to the image data file.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing image data with image ID as keys.\n",
    "              Each image entry contains camera ID and name.\n",
    "    \"\"\"\n",
    "    image_data = {}\n",
    "    try:\n",
    "        with open(image_file, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            for i in range(0, len(lines), 2):  # Process every two lines\n",
    "                image_info = lines[i].strip().split(' ')\n",
    "                image_id = int(image_info[0])\n",
    "                camera_id = int(image_info[-2])\n",
    "                name = image_info[-1]\n",
    "                image_data[image_id] = {'camera_id': camera_id, 'name': name}\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{image_file}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while parsing image data: {e}\")\n",
    "    return image_data\n",
    "\n",
    "def parse_point3d_data(points3d_file):\n",
    "    \"\"\"Parse 3D point data from a file.\n",
    "    \n",
    "    Args:\n",
    "        points3d_file (str): Path to the 3D point data file.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing 3D point data with point ID as keys.\n",
    "              Each point entry contains x, y, and z coordinates.\n",
    "    \"\"\"\n",
    "    point3d_data = {}\n",
    "    try:\n",
    "        with open(points3d_file, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines[3:]:  # Skip the first three comment lines\n",
    "                parts = line.strip().split(' ')\n",
    "                point3d_id = int(parts[0])\n",
    "                x = float(parts[1])\n",
    "                y = float(parts[2])\n",
    "                z = float(parts[3])\n",
    "                point3d_data[point3d_id] = {'x': x, 'y': y, 'z': z}\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{points3d_file}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while parsing 3D point data: {e}\")\n",
    "    return point3d_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geometric_verification(matches, keypoints1, keypoints2, camera_poses):\n",
    "    \"\"\"Performs geometric verification of feature matches using epipolar constraint.\n",
    "\n",
    "    Args:\n",
    "        matches (list): List of matches between keypoints.\n",
    "        keyframes1 (list): List of keypoints for the first image.\n",
    "        keyframes2 (list): List of keypoints for the second image.\n",
    "        camera_poses (dict): Dictionary containing camera poses for each image.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of verified matches that pass the epipolar constraint.\n",
    "    \"\"\"\n",
    "\n",
    "    verified_matches = []\n",
    "    fx = camera_poses[image_paths[0]]['fx']  # Assuming focal length is available in camera data\n",
    "    fy = camera_poses[image_paths[0]]['fy']  # Assuming focal length is available in camera data\n",
    "    cx = camera_poses[image_paths[0]]['cx']  # Assuming principal point is available in camera data\n",
    "    cy = camera_poses[image_paths[0]]['cy']  # Assuming principal point is available in camera data\n",
    "\n",
    "    for match in matches:\n",
    "        # Extract keypoint indices from the match\n",
    "        query_idx = match.queryIdx\n",
    "        train_idx = match.trainIdx\n",
    "\n",
    "        # Get keypoint locations from keypoint lists\n",
    "        query_pt = keypoints1[query_idx].pt\n",
    "        train_pt = keypoints2[train_idx].pt\n",
    "\n",
    "        # Extract camera poses for the image pair\n",
    "        camera_pose1 = camera_poses[image_paths[0]]\n",
    "        camera_pose2 = camera_poses[image_paths[1]]\n",
    "\n",
    "        # Convert camera orientations from quaternions to rotation matrices\n",
    "        R1 = R.from_quat(camera_pose1['orientation'])\n",
    "        R2 = R.from_quat(camera_pose2['orientation'])\n",
    "\n",
    "        # Calculate 3D world point corresponding to the query keypoint in the first image\n",
    "        world_pt = np.linalg.inv(np.dot(R1.as_matrix(), np.array([[camera_poses[image_paths[0]]['location'][0],\n",
    "                                                                   camera_poses[image_paths[0]]['location'][1],\n",
    "                                                                   camera_poses[image_paths[0]]['location'][2],\n",
    "                                                                   1]]).T)) * np.dot(R1.as_matrix(), np.array([[(query_pt[0] - cx) / fx],\n",
    "                                                                                                   [(query_pt[1] - cy) / fy],\n",
    "                                                                                                   [1],\n",
    "                                                                                                   [0]]).T)\n",
    "\n",
    "        # Project the 3D world point back onto the second image plane using the second camera pose\n",
    "        projected_pt = np.dot(R2.as_matrix().T, (np.dot(camera_pose2['calibration_matrix'], world_pt) - np.array([camera_pose2['location'][0], camera_pose2['location'][1], camera_pose2['location'][2], 1]).T))[:2] / projected_pt[2]\n",
    "\n",
    "        # Check if the distance between the projected point and the train point is within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_matches_lowe(matches, descriptors1, descriptors2):\n",
    "    \"\"\"Refines feature matches using Lowe's ratio test.\n",
    "\n",
    "    Args:\n",
    "        matches (list): List of matches between keypoints.\n",
    "        descriptors1 (list): List of descriptors for the first image.\n",
    "        descriptors2 (list): List of descriptors for the second image.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of refined matches that pass the Lowe's ratio test.\n",
    "    \"\"\"\n",
    "\n",
    "    refined_matches = []\n",
    "    ratio_thresh = 0.8  # Adjust threshold as needed\n",
    "\n",
    "    for match in matches:\n",
    "        # Get the two best matches for the query descriptor\n",
    "        first_match = match\n",
    "        second_match_idx = min(i for i in range(len(matches)) if i != match.queryIdx and matches[i].trainIdx == match.trainIdx)\n",
    "        second_match = matches[second_match_idx]\n",
    "\n",
    "        # Calculate the distance ratio\n",
    "        distance_ratio = match.distance / second_match.distance\n",
    "\n",
    "        # If the ratio is greater than the threshold, consider it a good match\n",
    "        if distance_ratio > ratio_thresh:\n",
    "            refined_matches.append(match)\n",
    "\n",
    "    return refined_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_matches_ransac(matches, keypoints1, keypoints2, camera_poses, ransac_thresh=3.0):\n",
    "  \"\"\"\n",
    "  Filters feature matches using RANSAC (Random Sample Consensus).\n",
    "\n",
    "  Args:\n",
    "      matches (list): List of matches between keypoints.\n",
    "      keyframes1 (list): List of keypoints for the first image.\n",
    "      keyframes2 (list): List of keypoints for the second image.\n",
    "      camera_poses (dict): Dictionary containing camera poses for each image.\n",
    "      ransac_thresh (float, optional): Threshold for inlier distance in RANSAC. Defaults to 3.0.\n",
    "\n",
    "  Returns:\n",
    "      list: A list of inlier matches that pass the RANSAC test.\n",
    "  \"\"\"\n",
    "\n",
    "  # Import necessary libraries (assuming OpenCV is already imported)\n",
    "  import cv2\n",
    "\n",
    "  inlier_matches = []\n",
    "  if len(matches) > 4:  # Require at least 4 matches for RANSAC\n",
    "      # Extract keypoint locations for all matches\n",
    "      src_pts = np.float32([keypoints1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "      dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "      # Estimate homography (or fundamental matrix for uncalibrated cameras)\n",
    "      model, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, ransac_thresh)\n",
    "\n",
    "      # Filter inliers based on the homography and distance threshold\n",
    "      for i, m in enumerate(matches):\n",
    "          if mask[i][0]:  # Check if the match is an inlier\n",
    "              # Project the source point back onto the second image using the homography\n",
    "              projected_pt = cv2.perspectiveTransform(src_pts[i].reshape(-1, 1, 2), model)\n",
    "              # Calculate the distance between the projected point and the matched point\n",
    "              distance = np.linalg.norm(projected_pt[0][0] - dst_pts[i])\n",
    "              # Add the inlier match if the distance is within a threshold\n",
    "              if distance < ransac_thresh:\n",
    "                  inlier_matches.append(m)\n",
    "\n",
    "  return inlier_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangulate_points(inlier_matches, keypoints1, keypoints2, camera_poses):\n",
    "  \"\"\"\n",
    "  Reconstructs 3D points from inlier matches using triangulation.\n",
    "\n",
    "  Args:\n",
    "      inlier_matches (list): List of inlier matches after RANSAC filtering.\n",
    "      keyframes1 (list): List of keypoints for the first image.\n",
    "      keyframes2 (list): List of keypoints for the second image.\n",
    "      camera_poses (dict): Dictionary containing camera poses for each image.\n",
    "\n",
    "  Returns:\n",
    "      list: A list of reconstructed 3D points.\n",
    "  \"\"\"\n",
    "\n",
    "  # Import libraries (assuming NumPy is already imported)\n",
    "  import numpy as np\n",
    "\n",
    "  # Initialize empty list for 3D points\n",
    "  points3d = []\n",
    "\n",
    "  for match in inlier_matches:\n",
    "      # Extract corresponding keypoints from both images\n",
    "      query_pt = keypoints1[match.queryIdx].pt\n",
    "      train_pt = keypoints2[match.trainIdx].pt\n",
    "\n",
    "      # Extract camera projection matrices from camera poses\n",
    "      P1 = camera_poses[match.imgIdx1]  # Assuming camera poses are stored by image index\n",
    "      P2 = camera_poses[match.imgIdx2]\n",
    "\n",
    "      # Homogeneous coordinates for the corresponding keypoints\n",
    "      q = np.array([query_pt[0], query_pt[1], 1.0])\n",
    "      p = np.array([train_pt[0], train_pt[1], 1.0])\n",
    "\n",
    "      # Triangulation using linear least squares solution\n",
    "      A = np.vstack([np.cross(P1[2], P1[1] * p[0] - P1[0] * p[1]),\n",
    "                     np.cross(P2[2], P2[1] * q[0] - P2[0] * q[1])])\n",
    "      _, _, V = np.linalg.svd(A)\n",
    "      X = V[-1, :] / V[-1, -1]  # Normalized homogeneous coordinates of the 3D point\n",
    "\n",
    "      # Convert to non-homogeneous coordinates\n",
    "      points3d.append(X[:3])\n",
    "\n",
    "  return points3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_surface_mesh(points3d):\n",
    "  \"\"\"\n",
    "  Generates a surface mesh from a set of 3D points using Poisson surface reconstruction.\n",
    "\n",
    "  Args:\n",
    "      points3d (list): List of reconstructed 3D points.\n",
    "\n",
    "  Returns:\n",
    "      tuple: A tuple containing the vertices and faces of the surface mesh.\n",
    "  \"\"\"\n",
    "\n",
    "  # Import libraries (assuming PyMesh is already installed)\n",
    "  try:\n",
    "    import pymesh\n",
    "  except ImportError:\n",
    "    print(\"PyMesh library not found. Please install it for surface meshing functionality.\")\n",
    "    return None\n",
    "\n",
    "  # Convert points to PyMesh format (if PyMesh is available)\n",
    "  if pymesh:\n",
    "    verts = pymesh.utils.trimesh_to_vertex_list(np.array(points3d))\n",
    "\n",
    "    # Perform Poisson surface reconstruction\n",
    "    mesh = pymesh.generate_poisson_mesh(verts)\n",
    "\n",
    "    # Extract vertices and faces\n",
    "    vertices = mesh.vertices\n",
    "    faces = mesh.faces\n",
    "\n",
    "    return vertices, faces\n",
    "  else:\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reconstruction(points3d, camera_poses, K):\n",
    "  \"\"\"\n",
    "  Visualizes the reconstructed 3D points and camera frustums.\n",
    "\n",
    "  Args:\n",
    "      points3d (list): List of reconstructed 3D points.\n",
    "      camera_poses (dict): Dictionary containing camera poses for each image.\n",
    "      K (np.ndarray): Camera intrinsic matrix.\n",
    "  \"\"\"\n",
    "\n",
    "  import cv2\n",
    "\n",
    "  # Project 3D points onto the image plane of the first camera\n",
    "  projected_points = np.dot(K, (points3d.T @ camera_poses[0][:3, :3]))[:2, :] / projected_points[2, :]\n",
    "\n",
    "  # Draw points and camera frustums on a blank image\n",
    "  image = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "  for pt in projected_points.T:\n",
    "      cv2.circle(image, (int(pt[0]), int(pt[1])), 3, (0, 255, 0), -1)\n",
    "\n",
    "  # Assuming camera poses are represented by rotation and translation vectors\n",
    "  for R, t in camera_poses.values():\n",
    "      # Calculate camera frustum corners in world coordinates\n",
    "      frustum_points = cv2.projectPoints(np.float32([[0.5, 0.5, 0.1], [-0.5, 0.5, 0.1],\n",
    "                                                    [-0.5, -0.5, 0.1], [0.5, -0.5, 0.1],\n",
    "                                                    [0.5, 0.5, 1.0], [-0.5, 0.5, 1.0],\n",
    "                                                    [-0.5, -0.5, 1.0], [0.5, -0.5, 1.0]]),\n",
    "                                         R, t, K)[0]\n",
    "\n",
    "      # Project frustum corners onto the image plane\n",
    "      frustum_points = np.dot(K, (frustum_points.T @ camera_poses[0][:3, :3]))[:2, :] / frustum_points[2, :]\n",
    "\n",
    "      # Draw camera frustum\n",
    "      cv2.polylines(image, [np.int32(frustum_points)], True, (255, 0, 0), 2)\n",
    "\n",
    "  # Display the visualization\n",
    "  cv2.imshow(\"3D Reconstruction Visualization\", image)\n",
    "  cv2.waitKey(0)\n",
    "  cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 204\u001b[0m\n\u001b[0;32m    202\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m keypoints, descriptors\n\u001b[0;32m    203\u001b[0m \u001b[38;5;66;03m# Extract keypoints and descriptors from the first and second images (replace with your implementation)\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m keypoints1, descriptors1 \u001b[38;5;241m=\u001b[39m detect_and_compute_features(\u001b[43mimage1\u001b[49m)\n\u001b[0;32m    205\u001b[0m keypoints2, descriptors2 \u001b[38;5;241m=\u001b[39m detect_and_compute_features(image2)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# Refine feature matches using Lowe's ratio test\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image1' is not defined"
     ]
    }
   ],
   "source": [
    "def refine_matches_lowe(matches, descriptors1, descriptors2):\n",
    "    \"\"\"Refines feature matches using Lowe's ratio test.\n",
    "\n",
    "    Args:\n",
    "        matches (list): List of matches between keypoints.\n",
    "        descriptors1 (list): List of descriptors for the first image.\n",
    "        descriptors2 (list): List of descriptors for the second image.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of refined matches that pass the Lowe's ratio test.\n",
    "    \"\"\"\n",
    "\n",
    "    refined_matches = []\n",
    "    ratio_thresh = 0.8  # Adjust threshold as needed\n",
    "\n",
    "    for match in matches:\n",
    "        # Get the two best matches for the query descriptor\n",
    "        first_match = match\n",
    "        second_match_idx = min(i for i in range(len(matches)) if i != match.queryIdx and matches[i].trainIdx == match.trainIdx)\n",
    "        second_match = matches[second_match_idx]\n",
    "\n",
    "        # Calculate the distance ratio\n",
    "        distance_ratio = match.distance / second_match.distance\n",
    "\n",
    "        # If the ratio is greater than the threshold, consider it a good match\n",
    "        if distance_ratio > ratio_thresh:\n",
    "            refined_matches.append(match)\n",
    "\n",
    "    return refined_matches\n",
    "\n",
    "def filter_matches_ransac(matches, keypoints1, keypoints2, camera_poses, ransac_thresh=3.0):\n",
    "  \"\"\"\n",
    "  Filters feature matches using RANSAC (Random Sample Consensus).\n",
    "\n",
    "  Args:\n",
    "      matches (list): List of matches between keypoints.\n",
    "      keyframes1 (list): List of keypoints for the first image.\n",
    "      keyframes2 (list): List of keypoints for the second image.\n",
    "      camera_poses (dict): Dictionary containing camera poses for each image.\n",
    "      ransac_thresh (float, optional): Threshold for inlier distance in RANSAC. Defaults to 3.0.\n",
    "\n",
    "  Returns:\n",
    "      list: A list of inlier matches that pass the RANSAC test.\n",
    "  \"\"\"\n",
    "\n",
    "  # Import necessary libraries (assuming OpenCV is already imported)\n",
    "  import cv2\n",
    "\n",
    "  inlier_matches = []\n",
    "  if len(matches) > 4:  # Require at least 4 matches for RANSAC\n",
    "      # Extract keypoint locations for all matches\n",
    "      src_pts = np.float32([keypoints1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "      dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "      # Estimate homography (or fundamental matrix for uncalibrated cameras)\n",
    "      model, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, ransac_thresh)\n",
    "\n",
    "      # Filter inliers based on the homography and distance threshold\n",
    "      for i, m in enumerate(matches):\n",
    "          if mask[i][0]:  # Check if the match is an inlier\n",
    "              # Project the source point back onto the second image using the homography\n",
    "              projected_pt = cv2.perspectiveTransform(src_pts[i].reshape(-1, 1, 2), model)\n",
    "              # Calculate the distance between the projected point and the matched point\n",
    "              distance = np.linalg.norm(projected_pt[0][0] - dst_pts[i])\n",
    "              # Add the inlier match if the distance is within a threshold\n",
    "              if distance < ransac_thresh:\n",
    "                  inlier_matches.append(m)\n",
    "\n",
    "  return inlier_matches\n",
    "\n",
    "def triangulate_points(inlier_matches, keypoints1, keypoints2, camera_poses):\n",
    "  \"\"\"\n",
    "  Reconstructs 3D points from inlier matches using triangulation.\n",
    "\n",
    "  Args:\n",
    "      inlier_matches (list): List of inlier matches after RANSAC filtering.\n",
    "      keyframes1 (list): List of keypoints for the first image.\n",
    "      keyframes2 (list): List of keypoints for the second image.\n",
    "      camera_poses (dict): Dictionary containing camera poses for each image.\n",
    "\n",
    "  Returns:\n",
    "      list: A list of reconstructed 3D points.\n",
    "  \"\"\"\n",
    "\n",
    "  # Import libraries (assuming NumPy is already imported)\n",
    "  import numpy as np\n",
    "\n",
    "  # Initialize empty list for 3D points\n",
    "  points3d = []\n",
    "\n",
    "  for match in inlier_matches:\n",
    "      # Extract corresponding keypoints from both images\n",
    "      query_pt = keypoints1[match.queryIdx].pt\n",
    "      train_pt = keypoints2[match.trainIdx].pt\n",
    "\n",
    "      # Extract camera projection matrices from camera poses\n",
    "      P1 = camera_poses[match.imgIdx1]  # Assuming camera poses are stored by image index\n",
    "      P2 = camera_poses[match.imgIdx2]\n",
    "\n",
    "      # Homogeneous coordinates for the corresponding keypoints\n",
    "      q = np.array([query_pt[0], query_pt[1], 1.0])\n",
    "      p = np.array([train_pt[0], train_pt[1], 1.0])\n",
    "\n",
    "      # Triangulation using linear least squares solution\n",
    "      A = np.vstack([np.cross(P1[2], P1[1] * p[0] - P1[0] * p[1]),\n",
    "                     np.cross(P2[2], P2[1] * q[0] - P2[0] * q[1])])\n",
    "      _, _, V = np.linalg.svd(A)\n",
    "      X = V[-1, :] / V[-1, -1]  # Normalized homogeneous coordinates of the 3D point\n",
    "\n",
    "      # Convert to non-homogeneous coordinates\n",
    "      points3d.append(X[:3])\n",
    "\n",
    "  return points3d\n",
    "\n",
    "def poisson_surface_mesh(points3d):\n",
    "  \"\"\"\n",
    "  Generates a surface mesh from a set of 3D points using Poisson surface reconstruction.\n",
    "\n",
    "  Args:\n",
    "      points3d (list): List of reconstructed 3D points.\n",
    "\n",
    "  Returns:\n",
    "      tuple: A tuple containing the vertices and faces of the surface mesh (or None if PyMesh is unavailable).\n",
    "  \"\"\"\n",
    "\n",
    "  try:\n",
    "    import pymesh\n",
    "    verts = pymesh.utils.trimesh_to_vertex_list(np.array(points3d))\n",
    "    mesh = pymesh.generate_poisson_mesh(verts)\n",
    "    vertices = mesh.vertices\n",
    "    faces = mesh.faces\n",
    "    return vertices, faces\n",
    "  except ImportError:\n",
    "    print(\"PyMesh library not found. Surface meshing skipped.\")\n",
    "    return None\n",
    "\n",
    "# Function for visualization using OpenCV (unchanged)\n",
    "def visualize_reconstruction(points3d, camera_poses, K):\n",
    "  \"\"\"\n",
    "  Visualizes the reconstructed 3D points and camera frustums.\n",
    "\n",
    "  Args:\n",
    "      points3d (list): List of reconstructed 3D points.\n",
    "      camera_poses (dict): Dictionary containing camera poses for each image.\n",
    "      K (np.ndarray): Camera intrinsic matrix.\n",
    "  \"\"\"\n",
    "\n",
    "  import cv2\n",
    "\n",
    "  # Project 3D points onto the image plane of the first camera\n",
    "  projected_points = np.dot(K, (points3d.T @ camera_poses[0][:3, :3]))[:2, :] / projected_points[2, :]\n",
    "\n",
    "  # Draw points and camera frustums on a blank image\n",
    "  image = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "  for pt in projected_points.T:\n",
    "      cv2.circle(image, (int(pt[0]), int(pt[1])), 3, (0, 255, 0), -1)\n",
    "\n",
    "  # Assuming camera poses are represented by rotation and translation vectors\n",
    "  for R, t in camera_poses.values():\n",
    "      # Calculate camera frustum corners in world coordinates\n",
    "      frustum_points = cv2.projectPoints(np.float32([[0.5, 0.5, 0.1], [-0.5, 0.5, 0.1],\n",
    "                                                    [-0.5, -0.5, 0.1], [0.5, -0.5, 0.1],\n",
    "                                                    [0.5, 0.5, 1.0], [-0.5, 0.5, 1.0],\n",
    "                                                    [-0.5, -0.5, 1.0], [0.5, -0.5, 1.0]]),\n",
    "                                         R, t, K)[0]\n",
    "\n",
    "      # Project frustum corners onto the image plane\n",
    "      frustum_points = np.dot(K, (frustum_points.T @ camera_poses[0][:3, :3]))[:2, :] / frustum_points[2, :]\n",
    "\n",
    "      # Draw camera frustum\n",
    "      cv2.polylines(image, [np.int32(frustum_points)], True, (255, 0, 0), 2)\n",
    "\n",
    "  # Display the visualization\n",
    "  cv2.imshow(\"3D Reconstruction Visualization\", image)\n",
    "  cv2.waitKey(0)\n",
    "  cv2.destroyAllWindows()\n",
    "\n",
    "# Assuming you have your feature matching, camera poses, and intrinsic matrix K\n",
    "def detect_and_compute_features(image):\n",
    "  \"\"\"\n",
    "  Detects keypoints and computes descriptors for a given image.\n",
    "\n",
    "  Args:\n",
    "      image (np.ndarray): The input image as a NumPy array.\n",
    "\n",
    "  Returns:\n",
    "      tuple: A tuple containing the list of keypoints and the list of descriptors.\n",
    "  \"\"\"\n",
    "\n",
    "  # Import libraries\n",
    "  import cv2\n",
    "\n",
    "  # Convert image to grayscale (if necessary for your chosen detector)\n",
    "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "  # Create a SIFT or SURF detector (replace with your preference)\n",
    "  sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "  # Detect keypoints and compute descriptors\n",
    "  keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "\n",
    "  return keypoints, descriptors\n",
    "# Extract keypoints and descriptors from the first and second images (replace with your implementation)\n",
    "keypoints1, descriptors1 = detect_and_compute_features(image1)\n",
    "keypoints2, descriptors2 = detect_and_compute_features(image2)\n",
    "\n",
    "# Refine feature matches using Lowe's ratio test\n",
    "refined_matches = refine_matches_lowe(matches, descriptors1, descriptors2)\n",
    "\n",
    "# Filter inlier matches using RANSAC\n",
    "inlier_matches = filter_matches_ransac(refined_matches, keypoints1, keypoints2, camera_poses)\n",
    "\n",
    "# Triangulate 3D points from inlier matches\n",
    "points3d = triangulate_points(inlier_matches, keypoints1, keypoints2, camera_poses)\n",
    "\n",
    "# Generate surface mesh (if PyMesh is available)\n",
    "vertices, faces = poisson_surface_mesh(points3d)\n",
    "\n",
    "# Visualize the reconstructed scene\n",
    "visualize_reconstruction(points3d, camera_poses, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
